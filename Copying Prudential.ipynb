{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Generating dummies...\n",
      "Filling in missing values...\n",
      "Scaling...\n",
      "Input shape: 1077\n",
      "Adding Layer 0: 128\n",
      "Adding tanh layer\n",
      "Adding 0.5 dropout\n",
      "Adding Layer 1: 64\n",
      "Adding tanh layer\n",
      "Adding 0.5 dropout\n",
      "Training model...\n",
      "Epoch 1/50\n",
      "59381/59381 [==============================] - 27s - loss: 1.6995    \n",
      "Epoch 2/50\n",
      "59381/59381 [==============================] - 25s - loss: 1.5185    \n",
      "Epoch 3/50\n",
      "59381/59381 [==============================] - 22s - loss: 1.4566    \n",
      "Epoch 4/50\n",
      "59381/59381 [==============================] - 22s - loss: 1.4105    \n",
      "Epoch 5/50\n",
      "59381/59381 [==============================] - 22s - loss: 1.3758    \n",
      "Epoch 6/50\n",
      "59381/59381 [==============================] - 21s - loss: 1.3471    \n",
      "Epoch 7/50\n",
      "59381/59381 [==============================] - 21s - loss: 1.3223    \n",
      "Epoch 8/50\n",
      "59381/59381 [==============================] - 26s - loss: 1.3059    \n",
      "Epoch 9/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.2925    \n",
      "Epoch 10/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.2834    \n",
      "Epoch 11/50\n",
      "59381/59381 [==============================] - 22s - loss: 1.2745    \n",
      "Epoch 12/50\n",
      "59381/59381 [==============================] - 19s - loss: 1.2646    \n",
      "Epoch 13/50\n",
      "59381/59381 [==============================] - 19s - loss: 1.2600    \n",
      "Epoch 14/50\n",
      "59381/59381 [==============================] - 19s - loss: 1.2515    \n",
      "Epoch 15/50\n",
      "59381/59381 [==============================] - 19s - loss: 1.2434    \n",
      "Epoch 16/50\n",
      "59381/59381 [==============================] - 19s - loss: 1.2357    \n",
      "Epoch 17/50\n",
      "59381/59381 [==============================] - 21s - loss: 1.2356    \n",
      "Epoch 18/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.2290    \n",
      "Epoch 19/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.2251    \n",
      "Epoch 20/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.2246    \n",
      "Epoch 21/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.2233    \n",
      "Epoch 22/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.2190    \n",
      "Epoch 23/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.2112    \n",
      "Epoch 24/50\n",
      "59381/59381 [==============================] - 19s - loss: 1.2128    \n",
      "Epoch 25/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.2085    \n",
      "Epoch 26/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.2025    \n",
      "Epoch 27/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.2025    \n",
      "Epoch 28/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.2035    \n",
      "Epoch 29/50\n",
      "59381/59381 [==============================] - 21s - loss: 1.1976    \n",
      "Epoch 30/50\n",
      "59381/59381 [==============================] - 20s - loss: 1.1984    \n",
      "Epoch 31/50\n",
      "59381/59381 [==============================] - 16s - loss: 1.1959    \n",
      "Epoch 32/50\n",
      "59381/59381 [==============================] - 16s - loss: 1.1909    \n",
      "Epoch 33/50\n",
      "59381/59381 [==============================] - 16s - loss: 1.1883    \n",
      "Epoch 34/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1894    \n",
      "Epoch 35/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1797    \n",
      "Epoch 36/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1879    \n",
      "Epoch 37/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1812    \n",
      "Epoch 38/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1839    \n",
      "Epoch 39/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.1736    \n",
      "Epoch 40/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.1726    \n",
      "Epoch 41/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1743    \n",
      "Epoch 42/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1732    \n",
      "Epoch 43/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1725    \n",
      "Epoch 44/50\n",
      "59381/59381 [==============================] - 24s - loss: 1.1668    \n",
      "Epoch 45/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.1698    \n",
      "Epoch 46/50\n",
      "59381/59381 [==============================] - 16s - loss: 1.1718    \n",
      "Epoch 47/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.1704    \n",
      "Epoch 48/50\n",
      "59381/59381 [==============================] - 18s - loss: 1.1656    \n",
      "Epoch 49/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.1609    \n",
      "Epoch 50/50\n",
      "59381/59381 [==============================] - 17s - loss: 1.1652    \n",
      "Making predictions...\n",
      "19765/19765 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/tdevries/prudential-life-insurance-assessment/neural-network-example\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "class NN:\n",
    "    #I made a small wrapper for the Keras model to make it more scikit-learn like\n",
    "    #I think they have something like this built in already, oh well\n",
    "    #See http://keras.io/ for parameter options\n",
    "    def __init__(self, inputShape, layers, dropout = [], activation='tanh', init = 'uniform', loss = 'rmse', optimizer = 'adadelta', nb_epochs = 50, batch_size = 32, verbose = 1):\n",
    "\n",
    "        model = Sequential()\n",
    "        for i in range(len(layers)):\n",
    "            if i == 0:\n",
    "                print (\"Input shape: \" + str(inputShape))\n",
    "                print (\"Adding Layer \" + str(i) + \": \" + str(layers[i]))\n",
    "                model.add(Dense(layers[i], input_dim = inputShape, init = init))\n",
    "            else:\n",
    "                print (\"Adding Layer \" + str(i) + \": \" + str(layers[i]))\n",
    "                model.add(Dense(layers[i], init = init))\n",
    "            print (\"Adding \" + activation + \" layer\")\n",
    "            model.add(Activation(activation))\n",
    "            model.add(BatchNormalization())\n",
    "            if len(dropout) > i:\n",
    "                print (\"Adding \" + str(dropout[i]) + \" dropout\")\n",
    "                model.add(Dropout(dropout[i]))\n",
    "        model.add(Dense(1, init = init)) #End in a single output node for regression style output\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        self.model.fit(X.values, y.values, nb_epoch=self.nb_epochs, batch_size=self.batch_size, verbose = self.verbose)\n",
    "        \n",
    "    def predict(self, X, batch_size = 128, verbose = 1):\n",
    "        return self.model.predict(X.values, batch_size = batch_size, verbose = verbose)\n",
    "\n",
    "class pdStandardScaler:\n",
    "    #Applies the sklearn StandardScaler to pandas dataframes\n",
    "    def __init__(self):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        self.StandardScaler = StandardScaler()\n",
    "    def fit(self, df):\n",
    "        self.StandardScaler.fit(df)\n",
    "    def transform(self, df):\n",
    "        df = pd.DataFrame(self.StandardScaler.transform(df), columns=df.columns)\n",
    "        return df\n",
    "    def fit_transform(self, df):\n",
    "        df = pd.DataFrame(self.StandardScaler.fit_transform(df), columns=df.columns)\n",
    "        return df\n",
    "        \n",
    "def getDummiesInplace(columnList, train, test = None):\n",
    "    #Takes in a list of column names and one or two pandas dataframes\n",
    "    #One-hot encodes all indicated columns inplace\n",
    "    columns = []\n",
    "    \n",
    "    if test is not None:\n",
    "        df = pd.concat([train,test], axis= 0)\n",
    "    else:\n",
    "        df = train\n",
    "        \n",
    "    for columnName in df.columns:\n",
    "        index = df.columns.get_loc(columnName)\n",
    "        if columnName in columnList:\n",
    "            dummies = pd.get_dummies(df.ix[:,index], prefix = columnName, prefix_sep = \".\")\n",
    "            columns.append(dummies)\n",
    "        else:\n",
    "            columns.append(df.ix[:,index])\n",
    "    df = pd.concat(columns, axis = 1)\n",
    "    \n",
    "    if test is not None:\n",
    "        train = df[:train.shape[0]]\n",
    "        test = df[train.shape[0]:]\n",
    "        return train, test\n",
    "    else:\n",
    "        train = df\n",
    "        return train\n",
    "        \n",
    "def pdFillNAN(df, strategy = \"mean\"):\n",
    "    #Fills empty values with either the mean value of each feature, or an indicated number\n",
    "    if strategy == \"mean\":\n",
    "        return df.fillna(df.mean())\n",
    "    elif type(strategy) == int:\n",
    "        return df.fillna(strategy)\n",
    "        \n",
    "def make_dataset(useDummies = True, fillNANStrategy = \"mean\", useNormalization = True):\n",
    "    data_dir = \"/Users/davegelinas/GitHub/LearningMachines/Prudential Challenge/\"\n",
    "    train = pd.read_csv(data_dir + 'train.csv')\n",
    "    test = pd.read_csv(data_dir + 'test.csv')\n",
    "    \n",
    "    labels = train[\"Response\"]\n",
    "    train.drop(labels = \"Id\", axis = 1, inplace = True)\n",
    "    train.drop(labels = \"Response\", axis = 1, inplace = True)\n",
    "    test.drop(labels = \"Id\", axis = 1, inplace = True)\n",
    "    \n",
    "    categoricalVariables = [\"Product_Info_1\", \"Product_Info_2\", \"Product_Info_3\", \"Product_Info_5\", \"Product_Info_6\", \"Product_Info_7\", \"Employment_Info_2\", \"Employment_Info_3\", \"Employment_Info_5\", \"InsuredInfo_1\", \"InsuredInfo_2\", \"InsuredInfo_3\", \"InsuredInfo_4\", \"InsuredInfo_5\", \"InsuredInfo_6\", \"InsuredInfo_7\", \"Insurance_History_1\", \"Insurance_History_2\", \"Insurance_History_3\", \"Insurance_History_4\", \"Insurance_History_7\", \"Insurance_History_8\", \"Insurance_History_9\", \"Family_Hist_1\", \"Medical_History_2\", \"Medical_History_3\", \"Medical_History_4\", \"Medical_History_5\", \"Medical_History_6\", \"Medical_History_7\", \"Medical_History_8\", \"Medical_History_9\", \"Medical_History_10\", \"Medical_History_11\", \"Medical_History_12\", \"Medical_History_13\", \"Medical_History_14\", \"Medical_History_16\", \"Medical_History_17\", \"Medical_History_18\", \"Medical_History_19\", \"Medical_History_20\", \"Medical_History_21\", \"Medical_History_22\", \"Medical_History_23\", \"Medical_History_25\", \"Medical_History_26\", \"Medical_History_27\", \"Medical_History_28\", \"Medical_History_29\", \"Medical_History_30\", \"Medical_History_31\", \"Medical_History_33\", \"Medical_History_34\", \"Medical_History_35\", \"Medical_History_36\", \"Medical_History_37\", \"Medical_History_38\", \"Medical_History_39\", \"Medical_History_40\", \"Medical_History_41\"]\n",
    "\n",
    "    if useDummies == True:\n",
    "        print (\"Generating dummies...\")\n",
    "        train, test = getDummiesInplace(categoricalVariables, train, test)\n",
    "    \n",
    "    if fillNANStrategy is not None:\n",
    "        print (\"Filling in missing values...\")\n",
    "        train = pdFillNAN(train, fillNANStrategy)\n",
    "        test = pdFillNAN(test, fillNANStrategy)\n",
    "\n",
    "    if useNormalization == True:\n",
    "        print (\"Scaling...\")\n",
    "        scaler = pdStandardScaler()\n",
    "        train = scaler.fit_transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    \n",
    "    return train, test, labels\n",
    "\n",
    "print (\"Creating dataset...\") \n",
    "train, test, labels = make_dataset(useDummies = True, fillNANStrategy = \"mean\", useNormalization = True)\n",
    "    \n",
    "clf = NN(inputShape = train.shape[1], layers = [128, 64], dropout = [0.5, 0.5], loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    "\n",
    "print (\"Training model...\")\n",
    "clf.fit(train, labels)\n",
    "\n",
    "print (\"Making predictions...\")\n",
    "pred = clf.predict(test)\n",
    "predClipped = np.clip(np.round(pred), 1, 8).astype(int) #Make the submissions within the accepted range\n",
    "\n",
    "submission = pd.read_csv('/Users/davegelinas/Desktop/Kaggle/Prudential/13:12:2015/test.csv')\n",
    "submission[\"Response\"] = predClipped\n",
    "submission.to_csv('NNSubmission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
